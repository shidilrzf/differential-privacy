{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Privacy applied for SGD\n",
    "\n",
    "\n",
    "- bounded sensitivity of each gradient: we need to limit how much each individual training point sampled in a minibatch can influence gradient computations and the resulting updates applied to model parameters. This can be done by clipping each gradient computed on each training point.\n",
    "\n",
    "- Random noise is sampled and added to the clipped gradients to make it statistically impossible to know whether or not a particular data point was included in the training dataset by comparing the updates SGD applies when it operates with or without this particular data point in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 250\n",
    "\n",
    "l2_norm_clip = 1.5 # gradient clip\n",
    "noise_multiplier = 1.3 #random noise added\n",
    "num_microbatches = 250\n",
    "learning_rate = 0.25\n",
    "\n",
    "assert batch_size % num_microbatches == 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(16, 8, strides=2, padding='same', activation='relu', input_shape=(28, 28, 1))\n",
    "        self.maxpool1 = layers.MaxPool2D(2, 1)\n",
    "        self.conv2 = layers.Conv2D(32, 4, strides=2, padding='valid', activation='relu')\n",
    "        self.maxpool2 = layers.MaxPool2D(2, 1)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # Fully connected layer.\n",
    "        self.fc1 = layers.Dense(32, activation='relu')\n",
    "        self.dropout = layers.Dropout(rate=0.5)\n",
    "        self.out = layers.Dense(10)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=is_training)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.3023 - accuracy: 0.1166 - val_loss: 2.3022 - val_accuracy: 0.1497\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3015 - accuracy: 0.1143 - val_loss: 2.2995 - val_accuracy: 0.1209\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3012 - accuracy: 0.1187 - val_loss: 2.3034 - val_accuracy: 0.1015\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3048 - accuracy: 0.1043 - val_loss: 2.3022 - val_accuracy: 0.1160\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3049 - accuracy: 0.1060 - val_loss: 2.3074 - val_accuracy: 0.1062\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3075 - accuracy: 0.1020 - val_loss: 2.3057 - val_accuracy: 0.1222\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3093 - accuracy: 0.1084 - val_loss: 2.3074 - val_accuracy: 0.1393\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3094 - accuracy: 0.1122 - val_loss: 2.3081 - val_accuracy: 0.1022\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3068 - accuracy: 0.1277 - val_loss: 2.3068 - val_accuracy: 0.1362\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3130 - accuracy: 0.1158 - val_loss: 2.3176 - val_accuracy: 0.1063\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3153 - accuracy: 0.1176 - val_loss: 2.3068 - val_accuracy: 0.1383\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3161 - accuracy: 0.1270 - val_loss: 2.3115 - val_accuracy: 0.1366\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3163 - accuracy: 0.1129 - val_loss: 2.3152 - val_accuracy: 0.1084\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3250 - accuracy: 0.0968 - val_loss: 2.3238 - val_accuracy: 0.0990\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3160 - accuracy: 0.1157 - val_loss: 2.3031 - val_accuracy: 0.1399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee646819e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_data, test_labels),\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.417% and noise_multiplier = 1.3 iterated over 3600 steps satisfies differential privacy with eps = 1.18 and delta = 1e-05.\n",
      "The optimal RDP order is 17.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1799006739827, 17.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=250, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
